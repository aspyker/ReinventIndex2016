<a>










<div id="session_7956" class="resultRow sessionRow ">
    <div class="detailColumn">



        <a href="sessionDetail.ww?SESSION_ID=7956" class="openInPopup">

            <span class="abbreviation">BDM201 - </span>

            <span class="title">Big Data Architectural Patterns and Best Practices on AWS


						<i class="fa fa-picture-o sessionVideo" aria-hidden="true"></i>
					</span>
        </a>


        <span class="abstract">The world is producing an ever increasing volume, velocity, and variety of big data. Consumers and businesses are demanding up-to-the-second (or even millisecond) analytics on their fast-moving data, in addition to classic batch processing. AWS delivers many technologies for solving big data problems. But what services should you use, why, when, and how? In this session, we simplify big data processing as a data bus comprising various stages: ingest, store, process, and visualize. Next, we discuss how to choose the right technology in each stage based on criteria such as data structure, query latency, cost, request rate, item size, data volume, durability, and so on. Finally, we provide reference architecture, design patterns, and best practices for assembling these technologies to solve your big data problems at the right cost.</span>


        <small class="length">1 Hour</small>


        <small class="type">Breakout Session</small>


        <span class="track"></span>
        <span class="scheduleStatus">




				</span>
    </div>
    <div class="actionColumn">







    </div>
</div>


<div id="session_11758" class="resultRow sessionRow ">
    <div class="detailColumn">



        <a href="sessionDetail.ww?SESSION_ID=11758" class="openInPopup">

            <span class="abbreviation">BDM201-R - </span>

            <span class="title">[REPEAT] Big Data Architectural Patterns and Best Practices on AWS

						<i class="fa fa-youtube-play sessionPhoto" aria-hidden="true"></i>

					</span>
        </a>


        <span class="abstract">The world is producing an ever increasing volume, velocity, and variety of big data. Consumers and businesses are demanding up-to-the-second (or even millisecond) analytics on their fast-moving data, in addition to classic batch processing. AWS delivers many technologies for solving big data problems. But what services should you use, why, when, and how? In this session, we simplify big data processing as a data bus comprising various stages: ingest, store, process, and visualize. Next, we discuss how to choose the right technology in each stage based on criteria such as data structure, query latency, cost, request rate, item size, data volume, durability, and so on. Finally, we provide reference architecture, design patterns, and best practices for assembling these technologies to solve your big data problems at the right cost.</span>


        <small class="length">1 Hour</small>


        <small class="type">Breakout Session</small>


        <span class="track"></span>
        <span class="scheduleStatus">




				</span>
    </div>
    <div class="actionColumn">







    </div>
</div>


<div id="session_7949" class="resultRow sessionRow ">
    <div class="detailColumn">



        <a href="sessionDetail.ww?SESSION_ID=7949" class="openInPopup">

            <span class="abbreviation">BDM202 - </span>

            <span class="title">Workshop: Building Your First Big Data Application with AWS


						<i class="fa fa-picture-o sessionVideo" aria-hidden="true"></i>
					</span>
        </a>


        <span class="abstract">Want to get ramped up on how to use Amazon's big data web services and launch your first big data application on AWS? Join us in this workshop as we build a big data application in real time using Amazon EMR, Amazon Redshift, Amazon Kinesis, Amazon DynamoDB, and Amazon S3. We review architecture design patterns for big data solutions on AWS, and give you access to a take-home lab so that you can rebuild and customize the application yourself.</span>


        <small class="length">2.5 hours</small>


        <small class="type">Workshop</small>


        <span class="track"></span>
        <span class="scheduleStatus">




				</span>
    </div>
    <div class="actionColumn">







    </div>
</div>


<div id="session_7952" class="resultRow sessionRow ">
    <div class="detailColumn">



        <a href="sessionDetail.ww?SESSION_ID=7952" class="openInPopup">

            <span class="abbreviation">BDM203 - </span>

            <span class="title">FINRA: Building a Secure Data Science Platform on AWS

						<i class="fa fa-youtube-play sessionPhoto" aria-hidden="true"></i>
						<i class="fa fa-picture-o sessionVideo" aria-hidden="true"></i>
					</span>
        </a>

        <label style="color:#0126f5;">Just Added!</label>


        <span class="abstract">Data science is a key discipline in a data-driven organization. Through analytics, data scientists can uncover previously unknown relationships in data to help an organization make better decisions.&nbsp;However, data science is often performed from local machines with limited resources and multiple datasets on a variety of databases. Moving to the cloud can help organizations provide scalable compute and storage resources to data scientists, while freeing them from the burden of setting up and managing infrastructure.

In this session, FINRA, the Financial Industry Regulatory Authority, shares best practices and lessons learned when building a self-service, curated data science platform on AWS.&nbsp; A project that allowed us to remove the technology middleman and empower users to choose the best compute environment for their workloads. Understand the architecture and underlying data infrastructure services to provide a secure, self-service portal to data scientists, learn how we built consensus for tooling from of our data science community, hear about the benefits of increased collaboration among the scientists due to the standardized tools, and learn how you can retain the freedom to experiment with the latest technologies while retaining information security boundaries within a virtual private cloud (VPC).</span>


        <small class="length">1 Hour</small>


        <small class="type">Breakout Session</small>


        <span class="track"></span>
        <span class="scheduleStatus">




				</span>
    </div>
    <div class="actionColumn">







    </div>
</div>


<div id="session_7957" class="resultRow sessionRow ">
    <div class="detailColumn">



        <a href="sessionDetail.ww?SESSION_ID=7957" class="openInPopup">

            <span class="abbreviation">BDM204 - </span>

            <span class="title">Visualizing Big Data Insights with Amazon QuickSight

						<i class="fa fa-youtube-play sessionPhoto" aria-hidden="true"></i>
						<i class="fa fa-picture-o sessionVideo" aria-hidden="true"></i>
					</span>
        </a>

        <label style="color:#0126f5;">Just Added!</label>


        <span class="abstract">Amazon QuickSight is a fast BI service that makes it easy for you to build visualizations, perform ad-hoc analysis, and quickly get business insights from your data.&nbsp; QuickSight is built to harness the power and scalability of the cloud, so you can easily run analysis on large datasets, and support hundreds of thousands of users.&nbsp; In this session, we&rsquo;ll demonstrate how you can easily get started with Amazon QuickSight, uploading files, connecting to S3 and Redshift and creating analyses from visualizations that are optimized based on the underlying data.&nbsp;&nbsp; Once we&rsquo;ve built our analysis and dashboard, we&rsquo;ll show you easy it is to share it with colleagues and stakeholders in just a few seconds.&nbsp; And with SPICE &ndash; QuckSight&rsquo;s in-memory calculation engine &ndash; you can go from data to insights, faster than ever.</span>


        <small class="length">1 Hour</small>


        <small class="type">Breakout Session</small>


        <span class="track"></span>
        <span class="scheduleStatus">




				</span>
    </div>
    <div class="actionColumn">







    </div>
</div>


<div id="session_10747" class="resultRow sessionRow ">
    <div class="detailColumn">



        <a href="sessionDetail.ww?SESSION_ID=10747" class="openInPopup">

            <span class="abbreviation">BDM205 - </span>

            <span class="title">Big Data Mini Con State of the Union

						<i class="fa fa-youtube-play sessionPhoto" aria-hidden="true"></i>
						<i class="fa fa-picture-o sessionVideo" aria-hidden="true"></i>
					</span>
        </a>

        <label style="color:#0126f5;">Just Added!</label>


        <span class="abstract">Join us for this general session where AWS big data experts present an in-depth look at the current state of big data. Learn about the latest big data trends and industry use cases. Hear how other organizations are using the AWS big data platform to innovate and remain competitive. Take a look at some of the most recent AWS big data announcements, as we kick off the Big Data re:Source Mini Con.</span>


        <small class="length">1 Hour</small>


        <small class="type">Breakout Session</small>


        <span class="track"></span>
        <span class="scheduleStatus">




				</span>
    </div>
    <div class="actionColumn">







    </div>
</div>


<div id="session_7939" class="resultRow sessionRow ">
    <div class="detailColumn">



        <a href="sessionDetail.ww?SESSION_ID=7939" class="openInPopup">

            <span class="abbreviation">BDM206 - </span>

            <span class="title">Understanding IoT Data: How to Leverage Amazon Kinesis in Building an IoT Analytics Platform on AWS

						<i class="fa fa-youtube-play sessionPhoto" aria-hidden="true"></i>
						<i class="fa fa-picture-o sessionVideo" aria-hidden="true"></i>
					</span>
        </a>


        <span class="abstract">The growing popularity and breadth of use cases for IoT are challenging the traditional thinking of how data is acquired, processed, and analyzed to quickly gain insights and act promptly. Today, the potential of this data remains largely untapped. In this session, we explore architecture patterns for building comprehensive IoT analytics solutions using AWS big data services. We walk through two production-ready implementations. First, we present an end-to-end solution using AWS IoT, Amazon Kinesis, and AWS Lambda. Next, Hello discusses their consumer IoT solution built on top of Amazon Kinesis,&nbsp;Amazon DynamoDB, and Amazon Redshift.</span>


        <small class="length">1 Hour</small>


        <small class="type">Breakout Session</small>


        <span class="track"></span>
        <span class="scheduleStatus">




				</span>
    </div>
    <div class="actionColumn">







    </div>
</div>


<div id="session_7962" class="resultRow sessionRow ">
    <div class="detailColumn">



        <a href="sessionDetail.ww?SESSION_ID=7962" class="openInPopup">

            <span class="abbreviation">BDM301 - </span>

            <span class="title">Best Practices for Apache Spark on Amazon EMR

						<i class="fa fa-youtube-play sessionPhoto" aria-hidden="true"></i>
						<i class="fa fa-picture-o sessionVideo" aria-hidden="true"></i>
					</span>
        </a>


        <span class="abstract">Organizations need to perform increasingly complex analysis on data &mdash; streaming analytics, ad-hoc querying, and predictive analytics &mdash; in order to get better customer insights and actionable business intelligence. Apache Spark has recently emerged as the framework of choice to address many of these challenges. In this session, we show you how to use Apache Spark on AWS to implement and scale common big data use cases such as real-time data processing, interactive data science, predictive analytics, and more. We talk about common architectures, best practices to quickly create Spark clusters using Amazon EMR, and ways to integrate Spark with other big data services in AWS. This session will feature DataXu, a provider of programmatic marketing and analytics software. DataXu will share how they architected their petabyte-scale ETL processing pipeline and data science workflows using Spark.</span>


        <small class="length">1 Hour</small>


        <small class="type">Breakout Session</small>


        <span class="track"></span>
        <span class="scheduleStatus">




				</span>
    </div>
    <div class="actionColumn">







    </div>
</div>


<div id="session_11549" class="resultRow sessionRow ">
    <div class="detailColumn">



        <a href="sessionDetail.ww?SESSION_ID=11549" class="openInPopup">

            <span class="abbreviation">BDM301-R - </span>

            <span class="title">[REPEAT] Best Practices for Apache Spark on Amazon EMR



					</span>
        </a>

        <label style="color:#0126f5;">Just Added!</label>


        <span class="abstract">Organizations need to perform increasingly complex analysis on data &mdash; streaming analytics, ad-hoc querying, and predictive analytics &mdash; in order to get better customer insights and actionable business intelligence. Apache Spark has recently emerged as the framework of choice to address many of these challenges. In this session, we show you how to use Apache Spark on AWS to implement and scale common big data use cases such as real-time data processing, interactive data science, predictive analytics, and more. We talk about common architectures, best practices to quickly create Spark clusters using Amazon EMR, and ways to integrate Spark with other big data services in AWS. This session will feature DataXu, a provider of programmatic marketing and analytics software. DataXu will share how they architected their petabyte-scale ETL processing pipeline and data science workflows using Spark.</span>


        <small class="length">1 Hour</small>


        <small class="type">Breakout Session</small>


        <span class="track"></span>
        <span class="scheduleStatus">




				</span>
    </div>
    <div class="actionColumn">







    </div>
</div>


<div id="session_7954" class="resultRow sessionRow ">
    <div class="detailColumn">



        <a href="sessionDetail.ww?SESSION_ID=7954" class="openInPopup">

            <span class="abbreviation">BDM302 - </span>

            <span class="title">Real-Time Data Exploration and Analytics with Amazon Elasticsearch Service and Kibana

						<i class="fa fa-youtube-play sessionPhoto" aria-hidden="true"></i>
						<i class="fa fa-picture-o sessionVideo" aria-hidden="true"></i>
					</span>
        </a>

        <label style="color:#0126f5;">Just Added!</label>


        <span class="abstract">Elasticsearch is a fully featured&nbsp;search engine used for real-time analytics, and Amazon Elasticsearch Service makes it easy to deploy Elasticsearch clusters on AWS. With Amazon ES, you can ingest and process billions of events per day, and explore the data using Kibana to discover patterns. In this session, we use Apache web logs as example and show you how to build an end-to-end analytics solution. First, we cover how to configure an Amazon ES cluster and ingest data into it using Amazon Kinesis Firehose. We look at best practices for choosing instance types, storage options, shard counts, and index rotations based on the throughput of incoming data. Then we demonstrate how to set up a Kibana dashboard and build custom dashboard widgets. Finally, we dive deep into the Elasticsearch query DSL and review approaches for generating custom, ad-hoc reports.</span>


        <small class="length">1 Hour</small>


        <small class="type">Breakout Session</small>


        <span class="track"></span>
        <span class="scheduleStatus">




				</span>
    </div>
    <div class="actionColumn">







    </div>
</div>


<div id="session_11554" class="resultRow sessionRow ">
    <div class="detailColumn">



        <a href="sessionDetail.ww?SESSION_ID=11554" class="openInPopup">

            <span class="abbreviation">BDM302-R - </span>

            <span class="title">[REPEAT] Real-Time Data Exploration and Analytics with Amazon Elasticsearch Service and Kibana



					</span>
        </a>


        <span class="abstract">Elasticsearch is a fully featured&nbsp;search engine used for real-time analytics, and Amazon Elasticsearch Service makes it easy to deploy Elasticsearch clusters on AWS. With Amazon ES, you can ingest and process billions of events per day, and explore the data using Kibana to discover patterns. In this session, we use Apache web logs as example and show you how to build an end-to-end analytics solution. First, we cover how to configure an Amazon ES cluster and ingest data into it using Amazon Kinesis Firehose. We look at best practices for choosing instance types, storage options, shard counts, and index rotations based on the throughput of incoming data. Then we demonstrate how to set up a Kibana dashboard and build custom dashboard widgets. Finally, we dive deep into the Elasticsearch query DSL and review approaches for generating custom, ad-hoc reports.</span>


        <small class="length">1 Hour</small>


        <small class="type">Breakout Session</small>


        <span class="track"></span>
        <span class="scheduleStatus">




				</span>
    </div>
    <div class="actionColumn">







    </div>
</div>


<div id="session_7951" class="resultRow sessionRow ">
    <div class="detailColumn">



        <a href="sessionDetail.ww?SESSION_ID=7951" class="openInPopup">

            <span class="abbreviation">BDM303 - </span>

            <span class="title">JustGiving: Serverless Data Pipelines, Event-Driven ETL, and Stream Processing

						<i class="fa fa-youtube-play sessionPhoto" aria-hidden="true"></i>
						<i class="fa fa-picture-o sessionVideo" aria-hidden="true"></i>
					</span>
        </a>

        <label style="color:#0126f5;">Just Added!</label>


        <span class="abstract">Organizations need to gain insight and knowledge from a growing number of Internet of Things (IoT), application programming interfaces (API), clickstreams, unstructured and log data sources. However, organizations are also often limited by legacy data warehouses and ETL processes that were designed for transactional data. Building scalable big data pipelines with automated extract-transform-load (ETL) and machine learning processes can address these limitations. JustGiving is the world&rsquo;s largest social platform for online giving. In this session, we describe how we created several scalable and loosely coupled event-driven ETL and ML pipelines as part of our in-house data science platform called RAVEN. You learn how to leverage AWS Lambda, Amazon S3, Amazon EMR, Amazon Kinesis, and other services to build serverless, event-driven, data and stream processing pipelines in your organization. We review common design patterns, lessons learned, and best practices, with a focus on serverless big data architectures with AWS Lambda.</span>


        <small class="length">1 Hour</small>


        <small class="type">Breakout Session</small>


        <span class="track"></span>
        <span class="scheduleStatus">




				</span>
    </div>
    <div class="actionColumn">







    </div>
</div>


<div id="session_11550" class="resultRow sessionRow ">
    <div class="detailColumn">



        <a href="sessionDetail.ww?SESSION_ID=11550" class="openInPopup">

            <span class="abbreviation">BDM303-R - </span>

            <span class="title">[REPEAT] JustGiving: Serverless Data Pipelines, Event-Driven ETL, and Stream Processing



					</span>
        </a>

        <label style="color:#0126f5;">Just Added!</label>


        <span class="abstract">Organizations need to gain insight and knowledge from a growing number of Internet of Things (IoT), application programming interfaces (API), clickstreams, unstructured and log data sources. However, organizations are also often limited by legacy data warehouses and ETL processes that were designed for transactional data. Building scalable big data pipelines with automated extract-transform-load (ETL) and machine learning processes can address these limitations. JustGiving is the world&rsquo;s largest social platform for online giving. In this session, we describe how we created several scalable and loosely coupled event-driven ETL and ML pipelines as part of our in-house data science platform called RAVEN. You learn how to leverage AWS Lambda, Amazon S3, Amazon EMR, Amazon Kinesis, and other services to build serverless, event-driven, data and stream processing pipelines in your organization. We review common design patterns, lessons learned, and best practices, with a focus on serverless big data architectures with AWS Lambda.</span>


        <small class="length">1 Hour</small>


        <small class="type">Breakout Session</small>


        <span class="track"></span>
        <span class="scheduleStatus">




				</span>
    </div>
    <div class="actionColumn">







    </div>
</div>


<div id="session_7953" class="resultRow sessionRow ">
    <div class="detailColumn">



        <a href="sessionDetail.ww?SESSION_ID=7953" class="openInPopup">

            <span class="abbreviation">BDM304 - </span>

            <span class="title">Analyzing Streaming Data in Real-time with Amazon Kinesis Analytics

						<i class="fa fa-youtube-play sessionPhoto" aria-hidden="true"></i>
						<i class="fa fa-picture-o sessionVideo" aria-hidden="true"></i>
					</span>
        </a>

        <label style="color:#0126f5;">Just Added!</label>


        <span class="abstract">As more and more organizations strive to gain real-time insights into their business, streaming data has become ubiquitous. Typical streaming data analytics solutions require specific skills and complex infrastructure. However, with Amazon Kinesis&nbsp;Analytics, you can analyze streaming data in real-time with standard SQL&mdash;there is no need to learn new programming languages or processing frameworks.

In this session, we dive deep into the capabilities of Amazon Kinesis Analytics using real-world examples. We&rsquo;ll present an end-to-end streaming data solution using Amazon Kinesis&nbsp;Streams for data ingestion, Amazon Kinesis&nbsp;Analytics for real-time processing, and Amazon Kinesis&nbsp;Firehose for persistence. We review in detail how to write SQL queries using streaming data and discuss best practices to optimize and monitor your Amazon Kinesis&nbsp;Analytics applications. Lastly, we discuss how to estimate the cost of the entire system.</span>


        <small class="length">1 Hour</small>


        <small class="type">Breakout Session</small>


        <span class="track"></span>
        <span class="scheduleStatus">




				</span>
    </div>
    <div class="actionColumn">







    </div>
</div>


<div id="session_11661" class="resultRow sessionRow ">
    <div class="detailColumn">



        <a href="sessionDetail.ww?SESSION_ID=11661" class="openInPopup">

            <span class="abbreviation">BDM304-R - </span>

            <span class="title">[REPEAT] Analyzing Streaming Data in Real-time with Amazon Kinesis Analytics



					</span>
        </a>


        <span class="abstract">As more and more organizations strive to gain real-time insights into their business, streaming data has become ubiquitous. Typical streaming data analytics solutions require specific skills and complex infrastructure. However, with Amazon Kinesis&nbsp;Analytics, you can analyze streaming data in real-time with standard SQL&mdash;there is no need to learn new programming languages or processing frameworks.

In this session, we dive deep into the capabilities of Amazon Kinesis Analytics using real-world examples. We&rsquo;ll present an end-to-end streaming data solution using Amazon Kinesis&nbsp;Streams for data ingestion, Amazon Kinesis&nbsp;Analytics for real-time processing, and Amazon Kinesis&nbsp;Firehose for persistence. We review in detail how to write SQL queries using streaming data and discuss best practices to optimize and monitor your Amazon Kinesis&nbsp;Analytics applications. Lastly, we discuss how to estimate the cost of the entire system.</span>


        <small class="length">1 Hour</small>


        <small class="type">Breakout Session</small>


        <span class="track"></span>
        <span class="scheduleStatus">




				</span>
    </div>
    <div class="actionColumn">







    </div>
</div>


<div id="session_7948" class="resultRow sessionRow ">
    <div class="detailColumn">



        <a href="sessionDetail.ww?SESSION_ID=7948" class="openInPopup">

            <span class="abbreviation">BDM306 - </span>

            <span class="title">Netflix: Using Amazon S3 as the fabric of our big data ecosystem

						<i class="fa fa-youtube-play sessionPhoto" aria-hidden="true"></i>
						<i class="fa fa-picture-o sessionVideo" aria-hidden="true"></i>
					</span>
        </a>


        <span class="abstract">Amazon S3 is the central data hub for Netflix's big data ecosystem. We currently have over 1.5 billion objects and 60+ PB of data stored in S3. As we ingest, transform, transport, and visualize data, we find this data naturally weaving in and out of S3. Amazon S3 provides us the flexibility to use an interoperable set of big data processing tools like Spark, Presto, Hive, and Pig. It serves as the hub for transporting data to additional data stores / engines like Teradata, Redshift, and Druid, as well as exporting data to reporting tools like Microstrategy and Tableau. Over time, we have built an ecosystem of services and tools to manage our data on S3. We have a federated metadata catalog service that keeps track of all our data. We have a set of data lifecycle management tools that expire data based on business rules and compliance. We also have a portal that allows users to see the cost and size of their data footprint. In this talk, we&rsquo;ll dive into these major uses of S3, as well as many smaller cases, where S3 smoothly addresses an important data infrastructure need. We will also provide solutions and methodologies on how you can build your own S3 big data hub.</span>


        <small class="length">1 Hour</small>


        <small class="type">Breakout Session</small>


        <span class="track"></span>
        <span class="scheduleStatus">




				</span>
    </div>
    <div class="actionColumn">







    </div>
</div>


<div id="session_11548" class="resultRow sessionRow ">
    <div class="detailColumn">



        <a href="sessionDetail.ww?SESSION_ID=11548" class="openInPopup">

            <span class="abbreviation">BDM306-R - </span>

            <span class="title">[REPEAT] Netflix: Using Amazon S3 as the fabric of our big data ecosystem



					</span>
        </a>


        <span class="abstract">Amazon S3 is the central data hub for Netflix's big data ecosystem. We currently have over 1.5 billion objects and 60+ PB of data stored in S3. As we ingest, transform, transport, and visualize data, we find this data naturally weaving in and out of S3. Amazon S3 provides us the flexibility to use an interoperable set of big data processing tools like Spark, Presto, Hive, and Pig. It serves as the hub for transporting data to additional data stores / engines like Teradata, Redshift, and Druid, as well as exporting data to reporting tools like Microstrategy and Tableau. Over time, we have built an ecosystem of services and tools to manage our data on S3. We have a federated metadata catalog service that keeps track of all our data. We have a set of data lifecycle management tools that expire data based on business rules and compliance. We also have a portal that allows users to see the cost and size of their data footprint. In this talk, we&rsquo;ll dive into these major uses of S3, as well as many smaller cases, where S3 smoothly addresses an important data infrastructure need. We will also provide solutions and methodologies on how you can build your own S3 big data hub.</span>


        <small class="length">1 Hour</small>


        <small class="type">Breakout Session</small>


        <span class="track"></span>
        <span class="scheduleStatus">




				</span>
    </div>
    <div class="actionColumn">







    </div>
</div>


<div id="session_7950" class="resultRow sessionRow ">
    <div class="detailColumn">



        <a href="sessionDetail.ww?SESSION_ID=7950" class="openInPopup">

            <span class="abbreviation">BDM401 - </span>

            <span class="title">Deep Dive: Amazon EMR Best Practices & Design Patterns

						<i class="fa fa-youtube-play sessionPhoto" aria-hidden="true"></i>
						<i class="fa fa-picture-o sessionVideo" aria-hidden="true"></i>
					</span>
        </a>

        <label style="color:#0126f5;">Just Added!</label>


        <span class="abstract">Amazon EMR is one of the largest Hadoop operators in the world. In this session, we introduce you to Amazon EMR design patterns such as using Amazon S3 instead of HDFS, taking advantage of both long and short-lived clusters, and other Amazon EMR architectural best practices. We talk about how to scale your cluster up or down dynamically and introduce you to ways you can fine-tune your cluster. We also share best practices to keep your Amazon EMR cluster cost-efficient. Finally, we dive into some of our recent launches to keep you current on our latest features. This session will feature Asurion, a provider of device protection and support services for over 280 million smartphones and other consumer electronics devices.&nbsp;Asurion will share how they architected their petabyte-scale data platform using Apache Hive, Apache Spark, and Presto on Amazon EMR.</span>


        <small class="length">1 Hour</small>


        <small class="type">Breakout Session</small>


        <span class="track"></span>
        <span class="scheduleStatus">




				</span>
    </div>
    <div class="actionColumn">







    </div>
</div>


<div id="session_7958" class="resultRow sessionRow ">
    <div class="detailColumn">



        <a href="sessionDetail.ww?SESSION_ID=7958" class="openInPopup">

            <span class="abbreviation">BDM402 - </span>

            <span class="title">Best Practices for Data Warehousing with Amazon Redshift

						<i class="fa fa-youtube-play sessionPhoto" aria-hidden="true"></i>
						<i class="fa fa-picture-o sessionVideo" aria-hidden="true"></i>
					</span>
        </a>


        <span class="abstract">Analyzing big data quickly and efficiently requires a data warehouse optimized to handle and scale for large datasets. Amazon Redshift is a fast, petabyte-scale data warehouse that makes it simple and cost-effective to analyze all of your data for a fraction of the cost of traditional data warehouses. In this session, we take an in-depth look at data warehousing with Amazon Redshift for big data analytics. We cover best practices to&nbsp;take advantage of Amazon Redshift's columnar technology and parallel processing capabilities to deliver high throughput and query performance. We also discuss how to design optimal schemas,&nbsp;load&nbsp;data efficiently, and use&nbsp;work load management.</span>


        <small class="length">1 Hour</small>


        <small class="type">Breakout Session</small>


        <span class="track"></span>
        <span class="scheduleStatus">




				</span>
    </div>
    <div class="actionColumn">







    </div>
</div>


<div id="session_7955" class="resultRow sessionRow ">
    <div class="detailColumn">



        <a href="sessionDetail.ww?SESSION_ID=7955" class="openInPopup">

            <span class="abbreviation">BDM403 - </span>

            <span class="title">Beeswax: Building a Real-Time Streaming Data Platform on AWS

						<i class="fa fa-youtube-play sessionPhoto" aria-hidden="true"></i>
						<i class="fa fa-picture-o sessionVideo" aria-hidden="true"></i>
					</span>
        </a>


        <span class="abstract">Amazon Kinesis is a platform of services for building real-time, streaming data applications in the cloud. Customers can use Amazon Kinesis to collect, stream, and process real-time data such as website clickstreams, financial transactions, social media feeds, application logs, location-tracking events, and more. In this session, we first cover best practices for building an end-to-end streaming data applications using Amazon Kinesis. Next, Beeswax, which provides real-time Bidder as a Service for programmatic digital advertising, will talk about how they built a feature-rich, real-time streaming data solution on AWS using Amazon Kinesis, Amazon Redshift, Amazon S3, Amazon EMR, and Apache Spark. Beeswax will discuss key components of their solution including scalable data capture, messaging hub for archival, data warehousing, near real-time analytics, and real-time alerting.</span>


        <small class="length">1 Hour</small>


        <small class="type">Breakout Session</small>


        <span class="track"></span>
        <span class="scheduleStatus">




				</span>
    </div>
    <div class="actionColumn">







    </div>
</div>


<div id="session_11833" class="resultRow sessionRow ">
    <div class="detailColumn">



        <a href="sessionDetail.ww?SESSION_ID=11833" class="openInPopup">

            <span class="abbreviation">BDM403-R - </span>

            <span class="title">[REPEAT] Beeswax: Building a Real-Time Streaming Data Platform on AWS



					</span>
        </a>


        <span class="abstract">Amazon Kinesis is a platform of services for building real-time, streaming data applications in the cloud. Customers can use Amazon Kinesis to collect, stream, and process real-time data such as website clickstreams, financial transactions, social media feeds, application logs, location-tracking events, and more. In this session, we first cover best practices for building an end-to-end streaming data applications using Amazon Kinesis. Next, Beeswax, which provides real-time Bidder as a Service for programmatic digital advertising, will talk about how they built a feature-rich, real-time streaming data solution on AWS using Amazon Kinesis, Amazon Redshift, Amazon S3, Amazon EMR, and Apache Spark. Beeswax will discuss key components of their solution including scalable data capture, messaging hub for archival, data warehousing, near real-time analytics, and real-time alerting.</span>


        <small class="length">1 Hour</small>


        <small class="type">Breakout Session</small>


        <span class="track"></span>
        <span class="scheduleStatus">




				</span>
    </div>
    <div class="actionColumn">







    </div>
</div>





<div id="downloadDocsDialog" title="Available Docs"></div>


<script type="text/javascript" charset="utf-8">

    //update search quantities
    updateSearchCount({
        attendee: '',
        session: '21',
        speaker: '',
        exhibitor: '',
        file: '0'
    });

    $(function(){
        sessionTooltip();
        downloadDocDialogInit();
        ratingInit();
    });
</script>
</a>